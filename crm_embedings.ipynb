{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARQL query to extract lead transitions\n",
    "query = \"\"\"\n",
    "PREFIX crm: <http://www.example.org/crm_detailed_ontology#>\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "SELECT ?lead_id ?lead_name ?params_id ?old_status_id ?old_status_name ?new_status_id ?new_status_name ?change_date ?item_id\n",
    "WHERE {\n",
    "    ?params_class rdf:type crm:ParamsClass .\n",
    "    ?lead crm:hasItemFromLead ?item .\n",
    "\n",
    "\n",
    "    ?lead crm:hasID ?lead_id .\n",
    "    ?lead crm:hasName ?lead_name .\n",
    "    ?params_class crm:hasID ?params_id .\n",
    "\n",
    "    ?params_class crm:belongFromParamsClassToOldStatus ?old_status .\n",
    "\n",
    "    ?old_status crm:hasID ?old_status_id .\n",
    "    ?old_status crm:hasName ?old_status_name .\n",
    "\n",
    "    ?params_class crm:belongFromParamsClassToNewStatus ?new_status .\n",
    "    ?new_status crm:hasID ?new_status_id .\n",
    "\n",
    "    ?new_status crm:hasName ?new_status_name .\n",
    "\n",
    "    ?item crm:hasID ?item_id .\n",
    "    ?item crm:hasDateModified ?change_date .\n",
    "\n",
    "}\n",
    "ORDER BY ?lead_id ?change_date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_features = \"\"\"\n",
    "PREFIX crm: <http://www.example.org/crm_detailed_ontology#>\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "SELECT ?lead_id ?lead_name ?created_at ?responsible_user_id (COUNT(?item) AS ?interaction_count)\n",
    "WHERE {\n",
    "    ?lead rdf:type crm:Lead .\n",
    "    ?lead crm:hasID ?lead_id .\n",
    "    ?lead crm:hasName ?lead_name .\n",
    "    ?lead crm:hasCreatedAt ?created_at .\n",
    "    OPTIONAL { ?lead crm:hasResponsibleUserID ?responsible_user_id . }\n",
    "    OPTIONAL { ?lead crm:hasItemFromLead ?item . }\n",
    "}\n",
    "GROUP BY ?lead_id ?lead_name ?created_at ?responsible_user_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF, RDFS, XSD\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "\n",
    "# Load the RDF graph\n",
    "g = Graph()\n",
    "g.parse('crm_graph.rdf', format='xml')\n",
    "\n",
    "# Define the namespace\n",
    "CRM = Namespace(\"http://www.example.org/crm_detailed_ontology#\")\n",
    "g.bind(\"crm\", CRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the status transitions query\n",
    "results = g.query(query)\n",
    "\n",
    "# Process the results\n",
    "transitions_data = []\n",
    "for row in results:\n",
    "    lead_id = row.lead_id.toPython()\n",
    "    lead_name = row.lead_name.toPython()\n",
    "    params_id = row.params_id.toPython()\n",
    "    old_status_id = row.old_status_id.toPython()\n",
    "    old_status_name = row.old_status_name.toPython()\n",
    "    new_status_id = row.new_status_id.toPython()\n",
    "    new_status_name = row.new_status_name.toPython()\n",
    "    change_date = row.change_date.toPython()\n",
    "    item_id = row.item_id.toPython()\n",
    "    \n",
    "    \n",
    "    transitions_data.append({\n",
    "        'lead_id': lead_id,\n",
    "        'lead_name': lead_name,\n",
    "        'params_id': params_id,\n",
    "        'old_status_id': old_status_id,\n",
    "        'old_status_name': old_status_name,\n",
    "        'new_status_id': new_status_id,\n",
    "        'new_status_name': new_status_name,\n",
    "        'change_date': change_date,\n",
    "        'item_id': item_id\n",
    "    })\n",
    "\n",
    "# Similarly execute and process the features query\n",
    "results_features = g.query(query_features)\n",
    "\n",
    "features_data = []\n",
    "for row in results_features:\n",
    "    lead_id = row.lead_id.toPython()\n",
    "    lead_name = row.lead_name.toPython()\n",
    "    created_at = row.created_at.toPython()\n",
    "    responsible_user_id = row.responsible_user_id.toPython() if row.responsible_user_id else None\n",
    "    interaction_count = int(row.interaction_count)\n",
    "    \n",
    "    features_data.append({\n",
    "        'lead_id': lead_id,\n",
    "        'lead_name': lead_name,\n",
    "        'created_at': created_at,\n",
    "        'responsible_user_id': responsible_user_id,\n",
    "        'interaction_count': interaction_count\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "df_transitions = pd.DataFrame(transitions_data)\n",
    "df_features = pd.DataFrame(features_data)\n",
    "\n",
    "# Merge data on 'lead_id'\n",
    "df_merged = pd.merge(df_transitions, df_features, on=['lead_id', 'lead_name'], how='left')\n",
    "\n",
    "# Ensure 'change_date' is in datetime format\n",
    "df_merged['change_date'] = pd.to_datetime(df_merged['change_date'])\n",
    "df_merged['created_at'] = pd.to_datetime(df_merged['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60388 entries, 0 to 60387\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   lead_id          60388 non-null  int64         \n",
      " 1   lead_name        60388 non-null  object        \n",
      " 2   params_id        60388 non-null  int64         \n",
      " 3   old_status_id    60388 non-null  int64         \n",
      " 4   old_status_name  60388 non-null  object        \n",
      " 5   new_status_id    60388 non-null  int64         \n",
      " 6   new_status_name  60388 non-null  object        \n",
      " 7   change_date      60388 non-null  datetime64[ns]\n",
      " 8   item_id          60388 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(5), object(3)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_transitions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_id</th>\n",
       "      <th>lead_name</th>\n",
       "      <th>params_id</th>\n",
       "      <th>old_status_id</th>\n",
       "      <th>old_status_name</th>\n",
       "      <th>new_status_id</th>\n",
       "      <th>new_status_name</th>\n",
       "      <th>change_date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>responsible_user_id</th>\n",
       "      <th>interaction_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24050537</td>\n",
       "      <td>Proskater.Ru</td>\n",
       "      <td>278</td>\n",
       "      <td>41159701</td>\n",
       "      <td>возвращение в работу</td>\n",
       "      <td>34649023</td>\n",
       "      <td>Клиент квалифицирован</td>\n",
       "      <td>2020-10-07 16:20:20</td>\n",
       "      <td>47620073</td>\n",
       "      <td>1970-01-01 00:00:01.602076820</td>\n",
       "      <td>11089494</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24050537</td>\n",
       "      <td>Proskater.Ru</td>\n",
       "      <td>278</td>\n",
       "      <td>41159701</td>\n",
       "      <td>возвращение в работу</td>\n",
       "      <td>34649023</td>\n",
       "      <td>Клиент квалифицирован</td>\n",
       "      <td>2020-10-07 16:20:20</td>\n",
       "      <td>47620074</td>\n",
       "      <td>1970-01-01 00:00:01.602076820</td>\n",
       "      <td>11089494</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24050537</td>\n",
       "      <td>Proskater.Ru</td>\n",
       "      <td>278</td>\n",
       "      <td>41159701</td>\n",
       "      <td>возвращение в работу</td>\n",
       "      <td>34649023</td>\n",
       "      <td>Клиент квалифицирован</td>\n",
       "      <td>2020-10-07 16:20:20</td>\n",
       "      <td>47620075</td>\n",
       "      <td>1970-01-01 00:00:01.602076820</td>\n",
       "      <td>11089494</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24050537</td>\n",
       "      <td>Proskater.Ru</td>\n",
       "      <td>278</td>\n",
       "      <td>41159701</td>\n",
       "      <td>возвращение в работу</td>\n",
       "      <td>34649023</td>\n",
       "      <td>Клиент квалифицирован</td>\n",
       "      <td>2020-10-07 16:20:20</td>\n",
       "      <td>47620076</td>\n",
       "      <td>1970-01-01 00:00:01.602076820</td>\n",
       "      <td>11089494</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24050537</td>\n",
       "      <td>Proskater.Ru</td>\n",
       "      <td>278</td>\n",
       "      <td>41159701</td>\n",
       "      <td>возвращение в работу</td>\n",
       "      <td>34649023</td>\n",
       "      <td>Клиент квалифицирован</td>\n",
       "      <td>2020-10-07 16:20:20</td>\n",
       "      <td>47620077</td>\n",
       "      <td>1970-01-01 00:00:01.602076820</td>\n",
       "      <td>11089494</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lead_id     lead_name  params_id  old_status_id       old_status_name  \\\n",
       "0  24050537  Proskater.Ru        278       41159701  возвращение в работу   \n",
       "1  24050537  Proskater.Ru        278       41159701  возвращение в работу   \n",
       "2  24050537  Proskater.Ru        278       41159701  возвращение в работу   \n",
       "3  24050537  Proskater.Ru        278       41159701  возвращение в работу   \n",
       "4  24050537  Proskater.Ru        278       41159701  возвращение в работу   \n",
       "\n",
       "   new_status_id        new_status_name         change_date   item_id  \\\n",
       "0       34649023  Клиент квалифицирован 2020-10-07 16:20:20  47620073   \n",
       "1       34649023  Клиент квалифицирован 2020-10-07 16:20:20  47620074   \n",
       "2       34649023  Клиент квалифицирован 2020-10-07 16:20:20  47620075   \n",
       "3       34649023  Клиент квалифицирован 2020-10-07 16:20:20  47620076   \n",
       "4       34649023  Клиент квалифицирован 2020-10-07 16:20:20  47620077   \n",
       "\n",
       "                     created_at  responsible_user_id  interaction_count  \n",
       "0 1970-01-01 00:00:01.602076820             11089494                200  \n",
       "1 1970-01-01 00:00:01.602076820             11089494                200  \n",
       "2 1970-01-01 00:00:01.602076820             11089494                200  \n",
       "3 1970-01-01 00:00:01.602076820             11089494                200  \n",
       "4 1970-01-01 00:00:01.602076820             11089494                200  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_id</th>\n",
       "      <th>params_id</th>\n",
       "      <th>old_status_id</th>\n",
       "      <th>new_status_id</th>\n",
       "      <th>change_date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>responsible_user_id</th>\n",
       "      <th>interaction_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.038800e+04</td>\n",
       "      <td>60388.000000</td>\n",
       "      <td>6.038800e+04</td>\n",
       "      <td>6.038800e+04</td>\n",
       "      <td>60388</td>\n",
       "      <td>6.038800e+04</td>\n",
       "      <td>60388</td>\n",
       "      <td>6.038800e+04</td>\n",
       "      <td>60388.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.698685e+07</td>\n",
       "      <td>264.870968</td>\n",
       "      <td>2.932182e+07</td>\n",
       "      <td>3.160860e+07</td>\n",
       "      <td>2023-10-21 21:50:59.080081920</td>\n",
       "      <td>4.636560e+07</td>\n",
       "      <td>1970-01-01 00:00:01.665628697</td>\n",
       "      <td>8.692636e+06</td>\n",
       "      <td>89.817248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.405054e+07</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.420000e+02</td>\n",
       "      <td>1.420000e+02</td>\n",
       "      <td>2020-10-07 16:20:20</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1970-01-01 00:00:01.602076820</td>\n",
       "      <td>7.737097e+06</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.569082e+07</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>3.463941e+07</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>2023-05-18 15:19:41</td>\n",
       "      <td>4.761819e+07</td>\n",
       "      <td>1970-01-01 00:00:01.629795367</td>\n",
       "      <td>7.737097e+06</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.773609e+07</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>3.463941e+07</td>\n",
       "      <td>3.464989e+07</td>\n",
       "      <td>2024-02-20 05:10:50.500000</td>\n",
       "      <td>4.761981e+07</td>\n",
       "      <td>1970-01-01 00:00:01.679552085</td>\n",
       "      <td>7.737097e+06</td>\n",
       "      <td>82.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.792520e+07</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>3.464903e+07</td>\n",
       "      <td>5.289391e+07</td>\n",
       "      <td>2024-08-22 12:20:54</td>\n",
       "      <td>4.762005e+07</td>\n",
       "      <td>1970-01-01 00:00:01.685633521</td>\n",
       "      <td>1.108949e+07</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.878820e+07</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>5.289391e+07</td>\n",
       "      <td>5.289391e+07</td>\n",
       "      <td>2024-12-19 16:17:26</td>\n",
       "      <td>4.762033e+07</td>\n",
       "      <td>1970-01-01 00:00:01.714049212</td>\n",
       "      <td>1.108949e+07</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.667223e+06</td>\n",
       "      <td>137.962507</td>\n",
       "      <td>1.672471e+07</td>\n",
       "      <td>2.138987e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.924573e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.446659e+06</td>\n",
       "      <td>60.319156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            lead_id     params_id  old_status_id  new_status_id  \\\n",
       "count  6.038800e+04  60388.000000   6.038800e+04   6.038800e+04   \n",
       "mean   2.698685e+07    264.870968   2.932182e+07   3.160860e+07   \n",
       "min    2.405054e+07     12.000000   1.420000e+02   1.420000e+02   \n",
       "25%    2.569082e+07    149.000000   3.463941e+07   1.430000e+02   \n",
       "50%    2.773609e+07    307.000000   3.463941e+07   3.464989e+07   \n",
       "75%    2.792520e+07    376.000000   3.464903e+07   5.289391e+07   \n",
       "max    2.878820e+07    490.000000   5.289391e+07   5.289391e+07   \n",
       "std    1.667223e+06    137.962507   1.672471e+07   2.138987e+07   \n",
       "\n",
       "                         change_date       item_id  \\\n",
       "count                          60388  6.038800e+04   \n",
       "mean   2023-10-21 21:50:59.080081920  4.636560e+07   \n",
       "min              2020-10-07 16:20:20  1.000000e+00   \n",
       "25%              2023-05-18 15:19:41  4.761819e+07   \n",
       "50%       2024-02-20 05:10:50.500000  4.761981e+07   \n",
       "75%              2024-08-22 12:20:54  4.762005e+07   \n",
       "max              2024-12-19 16:17:26  4.762033e+07   \n",
       "std                              NaN  6.924573e+06   \n",
       "\n",
       "                          created_at  responsible_user_id  interaction_count  \n",
       "count                          60388         6.038800e+04       60388.000000  \n",
       "mean   1970-01-01 00:00:01.665628697         8.692636e+06          89.817248  \n",
       "min    1970-01-01 00:00:01.602076820         7.737097e+06          14.000000  \n",
       "25%    1970-01-01 00:00:01.629795367         7.737097e+06          45.000000  \n",
       "50%    1970-01-01 00:00:01.679552085         7.737097e+06          82.000000  \n",
       "75%    1970-01-01 00:00:01.685633521         1.108949e+07          93.000000  \n",
       "max    1970-01-01 00:00:01.714049212         1.108949e+07         200.000000  \n",
       "std                              NaN         1.446659e+06          60.319156  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_new_status 5129\n",
      "current_new_status 1709\n",
      "current_new_status 1319\n",
      "current_new_status 2009\n",
      "current_new_status 659\n",
      "current_new_status 599\n",
      "current_new_status 839\n",
      "current_new_status 209\n",
      "current_new_status 809\n",
      "current_new_status 449\n",
      "current_new_status 1139\n",
      "current_new_status 269\n",
      "current_new_status 1829\n",
      "current_new_status 929\n",
      "current_new_status 839\n",
      "current_new_status 1019\n",
      "current_new_status 2129\n"
     ]
    }
   ],
   "source": [
    "# Define the time window in days\n",
    "time_window = pd.Timedelta(days=7)\n",
    "\n",
    "# Sort transitions by lead_id and change_date\n",
    "df_merged.sort_values(by=['lead_id', 'change_date'], inplace=True)\n",
    "\n",
    "# Initialize labels\n",
    "df_merged['label'] = 0\n",
    "\n",
    "# Group by lead\n",
    "for lead_id, group in df_merged.groupby('lead_id'):\n",
    "    transitions = group.reset_index()\n",
    "    df_sorted_transitions = transitions.sort_values(by=['change_date'], ascending=False)\n",
    "    # print(len(df_sorted_transitions['item_id'].sort_values().unique()))\n",
    "    # df_sorted_transitions.to_csv('out.csv', sep='\\t')\n",
    "    # break\n",
    "    counter = 0\n",
    "    for i in range(len(transitions)):\n",
    "        current_transition = transitions.loc[i]\n",
    "        current_date = current_transition['change_date']\n",
    "        current_new_status = current_transition['new_status_id']\n",
    "        if i + 1 < len(transitions):\n",
    "            next_transition = transitions.loc[i + 1]\n",
    "            next_date = next_transition['change_date']\n",
    "            next_new_status = next_transition['new_status_id']\n",
    "            \n",
    "            if next_new_status != current_new_status:\n",
    "                counter+=1\n",
    "                \n",
    "                time_diff = next_date - current_date\n",
    "                if time_diff <= time_window:\n",
    "                    # Lead moved to next status within time window\n",
    "                    df_merged.loc[current_transition['index'], 'label'] = 1\n",
    "                else:\n",
    "                    df_merged.loc[current_transition['index'], 'label'] = 0\n",
    "        else:\n",
    "            # No subsequent transition; label depends on business logic\n",
    "            df_merged.loc[current_transition['index'], 'label'] = 0\n",
    "    print(\"current_new_status\", counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kirsr\\AppData\\Local\\Temp\\ipykernel_2872\\2805160972.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_merged['time_since_last_change'].fillna(df_merged['time_since_creation'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Time since lead creation\n",
    "df_merged['time_since_creation'] = (df_merged['change_date'] - df_merged['created_at']).dt.total_seconds() / (3600*24)\n",
    "\n",
    "# Convert categorical variables\n",
    "df_merged['responsible_user_id'] = df_merged['responsible_user_id'].astype('category')\n",
    "df_merged['new_status_id'] = df_merged['new_status_id'].astype('category')\n",
    "\n",
    "# For time since last status change, we can calculate the difference between current and previous change dates\n",
    "df_merged['time_since_last_change'] = df_merged.groupby('lead_id')['change_date'].diff().dt.total_seconds() / (3600*24)\n",
    "# Fill NaN with time since creation for the first transition\n",
    "df_merged['time_since_last_change'].fillna(df_merged['time_since_creation'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to numerical codes\n",
    "df_merged['responsible_user_code'] = df_merged['responsible_user_id'].cat.codes\n",
    "df_merged['new_status_code'] = df_merged['new_status_id'].cat.codes\n",
    "\n",
    "# Select features and label\n",
    "feature_columns = [\n",
    "    'time_since_creation',\n",
    "    'time_since_last_change',\n",
    "    'interaction_count',\n",
    "    'responsible_user_code',\n",
    "    'new_status_code',\n",
    "    # Add other features if necessary\n",
    "]\n",
    "X = df_merged[feature_columns]\n",
    "y = df_merged['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdflib import Namespace\n",
    "\n",
    "# Ensure IDs are strings\n",
    "df_merged['lead_id'] = df_merged['lead_id'].astype(str)\n",
    "df_merged['old_status_id'] = df_merged['old_status_id'].astype(str)\n",
    "df_merged['new_status_id'] = df_merged['new_status_id'].astype(str)\n",
    "df_merged['responsible_user_id'] = df_merged['responsible_user_id'].astype(str)\n",
    "\n",
    "# Define the CRM namespace\n",
    "CRM = Namespace(\"http://www.example.org/crm_detailed_ontology#\")\n",
    "\n",
    "# Generate URIs for entities\n",
    "df_merged['lead_uri'] = df_merged['lead_id'].apply(lambda x: f\"Lead{x}\")\n",
    "df_merged['old_status_uri'] = df_merged['old_status_id'].apply(lambda x: f\"Status{x}\")\n",
    "df_merged['new_status_uri'] = df_merged['new_status_id'].apply(lambda x: f\"Status{x}\")\n",
    "df_merged['responsible_user_uri'] = df_merged['responsible_user_id'].apply(lambda x: f\"User{x}\" if x != 'nan' else None)\n",
    "\n",
    "# Build triples from the data\n",
    "triples = []\n",
    "\n",
    "for idx, row in df_merged.iterrows():\n",
    "    lead_uri = row['lead_uri']\n",
    "    old_status_uri = row['old_status_uri']\n",
    "    new_status_uri = row['new_status_uri']\n",
    "    responsible_user_uri = row['responsible_user_uri']\n",
    "\n",
    "    # Triple for old status\n",
    "    triples.append((lead_uri, 'changedFromStatus', old_status_uri))\n",
    "\n",
    "    # Triple for new status\n",
    "    triples.append((lead_uri, 'changedToStatus', new_status_uri))\n",
    "\n",
    "    # Triple for current status\n",
    "    triples.append((lead_uri, 'hasStatus', new_status_uri))\n",
    "\n",
    "    # Triple for responsible user (if available)\n",
    "    if responsible_user_uri is not None and responsible_user_uri != 'Usernan':\n",
    "        triples.append((lead_uri, 'hasResponsibleUser', responsible_user_uri))\n",
    "\n",
    "# Optionally, include interaction data or other features as triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using automatically assigned random_state=539197513\n",
      "Training epochs on cpu: 100%|██████████| 100/100 [01:25<00:00,  1.17epoch/s, loss=0.477, prev_loss=0.522]\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/123 [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu: 100%|██████████| 123/123 [00:00<00:00, 546triple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.27s seconds\n"
     ]
    }
   ],
   "source": [
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.pipeline import pipeline\n",
    "\n",
    "# Convert the list of triples to a NumPy array\n",
    "triples_array = np.array(triples, dtype=str)\n",
    "\n",
    "# Create a TriplesFactory from the constructed triples\n",
    "tf = TriplesFactory.from_labeled_triples(\n",
    "    triples=triples_array\n",
    ")\n",
    "\n",
    "# Split the TriplesFactory into training and testing splits\n",
    "training_tf, testing_tf = tf.split([0.8, 0.2])\n",
    "\n",
    "# Train the embedding model using PyKEEN's pipeline\n",
    "result = pipeline(\n",
    "    training=training_tf,\n",
    "    testing=testing_tf,\n",
    "    model='TransE',\n",
    "    training_kwargs={\n",
    "        'num_epochs': 100,\n",
    "        'batch_size': 256\n",
    "    },\n",
    "    optimizer='adam',\n",
    "    optimizer_kwargs={\n",
    "        'lr': 0.001\n",
    "    },\n",
    "    random_seed=42,\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank (MR): 0.32\n",
      "Mean Reciprocal Rank (MRR): 0.2511\n",
      "Hits@1: 0.00\n",
      "Hits@3: 0.76\n",
      "Hits@10: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Retrieve evaluation results\n",
    "metric_results = result.metric_results\n",
    "\n",
    "# Extract metrics\n",
    "mr = metric_results.get_metric('adjusted_mean_rank')\n",
    "mrr = metric_results.get_metric('adjusted_mean_reciprocal_rank')\n",
    "hits_at_1 = metric_results.get_metric('hits_at_1')\n",
    "hits_at_3 = metric_results.get_metric('hits_at_3')\n",
    "hits_at_10 = metric_results.get_metric('hits_at_10')\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Mean Rank (MR): {mr:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "print(f\"Hits@1: {hits_at_1:.2f}\")\n",
    "print(f\"Hits@3: {hits_at_3:.2f}\")\n",
    "print(f\"Hits@10: {hits_at_10:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/123 [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu: 100%|██████████| 123/123 [00:00<00:00, 1.08ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 0.13s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rank (MR): 0.32\n",
      "Mean Reciprocal Rank (MRR): 0.2511\n",
      "Hits@1: 0.00\n",
      "Hits@3: 0.76\n",
      "Hits@10: 0.97\n"
     ]
    }
   ],
   "source": [
    "from pykeen.evaluation import RankBasedEvaluator\n",
    "\n",
    "# Define an evaluator\n",
    "evaluator = RankBasedEvaluator(filtered=True)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "metric_results = evaluator.evaluate(\n",
    "    model=result.model,\n",
    "    mapped_triples=testing_tf.mapped_triples,\n",
    "    additional_filter_triples=[training_tf.mapped_triples],\n",
    ")\n",
    "\n",
    "# Extract metrics\n",
    "mr = metric_results.get_metric('adjusted_mean_rank')\n",
    "mrr = metric_results.get_metric('adjusted_mean_reciprocal_rank')\n",
    "hits_at_1 = metric_results.get_metric('hits_at_1')\n",
    "hits_at_3 = metric_results.get_metric('hits_at_3')\n",
    "hits_at_10 = metric_results.get_metric('hits_at_10')\n",
    "\n",
    "# Display the metrics\n",
    "print(f\"Mean Rank (MR): {mr:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "print(f\"Hits@1: {hits_at_1:.2f}\")\n",
    "print(f\"Hits@3: {hits_at_3:.2f}\")\n",
    "print(f\"Hits@10: {hits_at_10:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, get the entity embeddings\n",
    "entity_embedding_model = result.model.entity_representations[0]\n",
    "entity_embeddings = entity_embedding_model().detach().cpu().numpy()\n",
    "\n",
    "# Get the mapping from entity labels to IDs\n",
    "entity_to_id = training_tf.entity_to_id\n",
    "id_to_entity = {idx: entity for entity, idx in entity_to_id.items()}\n",
    "\n",
    "# Create a dictionary mapping entity labels to embeddings\n",
    "entity_to_embedding = {\n",
    "    id_to_entity[idx]: embedding\n",
    "    for idx, embedding in enumerate(entity_embeddings)\n",
    "    if idx in id_to_entity\n",
    "}\n",
    "\n",
    "embedding_dimension = entity_embeddings.shape[1]\n",
    "\n",
    "# Function to get the embedding for a given entity URI\n",
    "def get_entity_embedding(entity_uri):\n",
    "    embedding = entity_to_embedding.get(entity_uri)\n",
    "    if embedding is not None:\n",
    "        return embedding\n",
    "    else:\n",
    "        return np.zeros(embedding_dimension)\n",
    "\n",
    "# Add lead embeddings to the DataFrame\n",
    "df_merged['lead_embedding'] = df_merged['lead_uri'].apply(get_entity_embedding)\n",
    "\n",
    "# Add status embeddings to the DataFrame\n",
    "df_merged['status_embedding'] = df_merged['new_status_uri'].apply(get_entity_embedding)\n",
    "\n",
    "# Encode categorical variables (if not already done)\n",
    "df_merged['responsible_user_id'] = df_merged['responsible_user_id'].astype('category')\n",
    "df_merged['responsible_user_code'] = df_merged['responsible_user_id'].cat.codes\n",
    "\n",
    "# Ensure feature columns are present\n",
    "feature_columns = [\n",
    "    'time_since_creation',\n",
    "    'time_since_last_change',\n",
    "    'interaction_count',\n",
    "    'responsible_user_code',\n",
    "    # Add other features as needed\n",
    "]\n",
    "\n",
    "# Combine embeddings and features\n",
    "def combine_features(row):\n",
    "    features = []\n",
    "    # Lead embedding\n",
    "    features.extend(row['lead_embedding'])\n",
    "    # Status embedding\n",
    "    features.extend(row['status_embedding'])\n",
    "    # Other numeric features\n",
    "    numeric_features = row[feature_columns].values.astype(float).tolist()\n",
    "    features.extend(numeric_features)\n",
    "    return features\n",
    "\n",
    "# Apply combine_features to each row\n",
    "df_merged['combined_features'] = df_merged.apply(combine_features, axis=1)\n",
    "\n",
    "# Prepare the final feature matrix X and target vector y\n",
    "X = np.vstack(df_merged['combined_features'].values)\n",
    "y = df_merged['label'].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        1.85426808e+04, 2.00000000e+02, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 2.00000000e+02, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 2.00000000e+02, 0.00000000e+00],\n",
       "       ...,\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 9.00000000e+01, 2.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 9.00000000e+01, 2.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 9.00000000e+01, 2.00000000e+00]],\n",
       "      shape=(60388, 104))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], shape=(60388,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.8039\n",
      "Accuracy: 0.7843\n",
      "Precision: 0.7645\n",
      "Recall (Sensitivity): 0.5833\n",
      "F1 Score: 0.6617\n",
      "\n",
      "Confusion Matrix:\n",
      "[[6925  785]\n",
      " [1820 2548]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.90      0.84      7710\n",
      "           1       0.76      0.58      0.66      4368\n",
      "\n",
      "    accuracy                           0.78     12078\n",
      "   macro avg       0.78      0.74      0.75     12078\n",
      "weighted avg       0.78      0.78      0.78     12078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Инициализация и обучение классификатора\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание вероятностей и меток на тестовой выборке\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Вычисление метрик\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Вывод метрик с автоматическим анализом\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Дополнительный отчет о классификации\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_transition_probability(deal, entity_to_embedding, model,\n",
    "                                   responsible_user_category_mapping,\n",
    "                                   new_status_category_mapping,\n",
    "                                   embedding_dimension):\n",
    "    \"\"\"\n",
    "    Predicts the probability of a lead transitioning to the next stage in the sales funnel.\n",
    "    \n",
    "    Parameters:\n",
    "        deal (dict): A dictionary containing the deal's features.\n",
    "        entity_to_embedding (dict): A mapping from entity URIs to embeddings.\n",
    "        model: The trained machine learning model.\n",
    "        responsible_user_category_mapping (dict): Mapping from responsible_user_id to codes.\n",
    "        new_status_category_mapping (dict): Mapping from status_id to codes.\n",
    "        embedding_dimension (int): Dimension of the embeddings.\n",
    "        \n",
    "    Returns:\n",
    "        float: The predicted probability of transitioning to the next stage.\n",
    "    \"\"\"\n",
    "    # Extract necessary fields from the deal\n",
    "    lead_id = str(deal.get('lead_id'))\n",
    "    current_status_id = str(deal.get('current_status_id'))\n",
    "    responsible_user_id = str(deal.get('responsible_user_id', 'UnknownUser'))\n",
    "    interaction_count = deal.get('interaction_count', 0)\n",
    "    created_at = pd.to_datetime(deal.get('created_at'))\n",
    "    change_date = pd.to_datetime(deal.get('change_date', datetime.now()))\n",
    "    last_change_date = pd.to_datetime(deal.get('last_change_date', created_at))\n",
    "    \n",
    "    # Compute features similar to the training data\n",
    "    \n",
    "    # Time since creation (in days)\n",
    "    time_since_creation = (change_date - created_at).total_seconds() / (3600 * 24)\n",
    "    \n",
    "    # Time since last change\n",
    "    time_since_last_change = (change_date - last_change_date).total_seconds() / (3600 * 24)\n",
    "    \n",
    "    # Encode responsible_user_id\n",
    "    if responsible_user_id in responsible_user_category_mapping:\n",
    "        responsible_user_code = responsible_user_category_mapping[responsible_user_id]\n",
    "    else:\n",
    "        responsible_user_code = max(responsible_user_category_mapping.values()) + 1  # Assign a new code\n",
    "    \n",
    "    # Encode current_status_id\n",
    "    if current_status_id in new_status_category_mapping:\n",
    "        new_status_code = new_status_category_mapping[current_status_id]\n",
    "    else:\n",
    "        new_status_code = max(new_status_category_mapping.values()) + 1  # Assign a new code\n",
    "    \n",
    "    # Generate entity URIs\n",
    "    lead_uri = f\"Lead{lead_id}\"\n",
    "    status_uri = f\"Status{current_status_id}\"\n",
    "    \n",
    "    # Retrieve embeddings\n",
    "    def get_entity_embedding(entity_uri):\n",
    "        embedding = entity_to_embedding.get(entity_uri)\n",
    "        if embedding is not None:\n",
    "            return embedding\n",
    "        else:\n",
    "            return np.zeros(embedding_dimension)\n",
    "    \n",
    "    lead_embedding = get_entity_embedding(lead_uri)\n",
    "    status_embedding = get_entity_embedding(status_uri)\n",
    "    \n",
    "    # Combine features\n",
    "    feature_columns = [\n",
    "        time_since_creation,\n",
    "        time_since_last_change,\n",
    "        interaction_count,\n",
    "        responsible_user_code,\n",
    "        new_status_code\n",
    "    ]\n",
    "    \n",
    "    combined_features = []\n",
    "    combined_features.extend(lead_embedding)\n",
    "    combined_features.extend(status_embedding)\n",
    "    combined_features.extend(feature_columns)\n",
    "    \n",
    "    # Convert to appropriate shape for model input\n",
    "    X_input = np.array(combined_features).reshape(1, -1)\n",
    "    \n",
    "    # Predict probability\n",
    "    probability = model.predict_proba(X_input)[0, 1]\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m new_status_category_mapping \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m new_status_category_mapping\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Predict the probability\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m probability \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_transition_probability\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity_to_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponsible_user_category_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_status_category_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_dimension\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobability\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[45], line 46\u001b[0m, in \u001b[0;36mpredict_transition_probability\u001b[1;34m(deal, entity_to_embedding, model, responsible_user_category_mapping, new_status_category_mapping, embedding_dimension)\u001b[0m\n\u001b[0;32m     44\u001b[0m     new_status_code \u001b[38;5;241m=\u001b[39m new_status_category_mapping[current_status_id]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     new_status_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_status_category_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m  \u001b[38;5;66;03m# Assign a new code\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Generate entity URIs\u001b[39;00m\n\u001b[0;32m     49\u001b[0m lead_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLead\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlead_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "# Prepare the necessary components\n",
    "embedding_dimension = next(iter(entity_to_embedding.values())).shape[0]\n",
    "\n",
    "# Extract category mappings and maximum codes from training data as shown earlier\n",
    "\n",
    "# Example deal\n",
    "deal = {\n",
    "    'lead_id': '123',\n",
    "    'current_status_id': '100',\n",
    "    'responsible_user_id': '42',\n",
    "    'interaction_count': 5,\n",
    "    'created_at': '2023-10-01T08:00:00',\n",
    "    'change_date': '2023-10-10T10:00:00',\n",
    "    'last_change_date': '2023-10-05T09:00:00'\n",
    "}\n",
    "\n",
    "df_merged['responsible_user_id'] = df_merged['responsible_user_id'].astype('category')\n",
    "responsible_user_category_mapping = dict(enumerate(df_merged['responsible_user_id'].cat.categories))\n",
    "responsible_user_category_mapping = {v: k for k, v in responsible_user_category_mapping.items()}\n",
    "\n",
    "# Encoding new_status_id\n",
    "df_merged['new_status_id'] = df_merged['new_status_id'].astype('category')\n",
    "new_status_category_mapping = dict(enumerate(df_merged['new_status_id'].cat.categories))\n",
    "new_status_category_mapping = {v: k for k, v in new_status_category_mapping.items()}\n",
    "\n",
    "new_status_category_mapping = {v: k for k, v in new_status_category_mapping.items()}\n",
    "\n",
    "# Predict the probability\n",
    "probability = predict_transition_probability(\n",
    "    deal,\n",
    "    entity_to_embedding,\n",
    "    model,\n",
    "    responsible_user_category_mapping,\n",
    "    new_status_category_mapping,\n",
    "    embedding_dimension\n",
    ")\n",
    "\n",
    "print(f\"Predicted Probability: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество триплетов в графе: 241552\n"
     ]
    }
   ],
   "source": [
    "total_triples = len(triples_array)\n",
    "print(f\"Общее количество триплетов в графе: {total_triples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество уникальных сущностей: 33\n"
     ]
    }
   ],
   "source": [
    "# Извлекаем все субъекты и объекты из триплетов\n",
    "entities = set(triples_array[:, 0]).union(set(triples_array[:, 2]))\n",
    "total_entities = len(entities)\n",
    "print(f\"Общее количество уникальных сущностей: {total_entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество уникальных отношений: 4\n"
     ]
    }
   ],
   "source": [
    "relations = set(triples_array[:, 1])\n",
    "total_relations = len(relations)\n",
    "print(f\"Общее количество уникальных отношений: {total_relations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная степень сущности: 49600\n",
      "Минимальная степень сущности: 868\n",
      "Средняя степень сущности: 14639.52\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Подсчёт количества связей для каждой сущности\n",
    "degree_count = Counter()\n",
    "for s, p, o in triples_array:\n",
    "    degree_count[s] += 1\n",
    "    degree_count[o] += 1\n",
    "\n",
    "# Статистика по степеням\n",
    "degrees = list(degree_count.values())\n",
    "max_degree = max(degrees)\n",
    "min_degree = min(degrees)\n",
    "average_degree = sum(degrees) / total_entities\n",
    "\n",
    "print(f\"Максимальная степень сущности: {max_degree}\")\n",
    "print(f\"Минимальная степень сущности: {min_degree}\")\n",
    "print(f\"Средняя степень сущности: {average_degree:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
